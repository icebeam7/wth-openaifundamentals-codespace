{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 04-B - Retrieval Augmented Generation (RAG) for Unstructured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Introduction\n",
    "\n",
    "Businesses have a lot of proprietary information that needs to be taken into account when answering user's questions - these cannot always be answered through the data that the GPT models have been trained on. \n",
    "\n",
    "In the last notebook, we worked with structured data primarily. A lot of the time, your enterprise data is not limited to just structured formats like CSV files or SQL tables. It may also include unstructured data like PDF documents or images. In fact, your individual documents could have both unstructured and structured data built into them. Extracting information from these diverse formats in a comprehensible manner presents a challenge. Tools like Azure Form Recognizer enable the extraction of data from unstructured sources such as forms or documents. Once the data is extracted into a structured JSON format, then Cognitive Search can be utilized to consolidate the entire information from different data types into indexes, facilitating the retrieval of relevant documents.\n",
    "\n",
    "In this notebook, we will walk you through a use case of Retrieval Augmented Generation (RAG) that involves working with unstructured data. The RAG approach combines various technologies to enhance the quality and relevance of generated outputs. We will leverage Azure Form Recognizer to process complex documents, utilizing the layout API to extract text and tables effectively. We will utilize Azure Cognitive Search to create an index by configuring semantic search capabilities, enabling the retrieval of relevant document pages. Additionally, embeddings will be incorporated to retrieve content that is more closely aligned with the user's question. Finally, Azure OpenAI's ChatGPT model will utilize the extracted content to generate a more meaningful answer. It is important to emphasize that this grounding process follows the RAG pattern mentioned in the previous notebook and helps eliminate inaccuracies in the generated responses.\n",
    "\n",
    "Your goals for this challenge are to read through this notebook, run each code block, observe the results, and then be able to answer the questions posed in the student guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken==0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from tiktoken==0.4.0) (2024.5.15)\n",
      "Requirement already satisfied: requests>=2.26.0 in /home/codespace/.local/lib/python3.10/site-packages (from tiktoken==0.4.0) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.26.0->tiktoken==0.4.0) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"tiktoken==0.4.0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-ai-formrecognizer==3.3.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (3.3.3)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from azure-ai-formrecognizer==3.3.3) (1.30.1)\n",
      "Requirement already satisfied: msrest>=0.6.21 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from azure-ai-formrecognizer==3.3.3) (0.7.1)\n",
      "Requirement already satisfied: azure-common>=1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from azure-ai-formrecognizer==3.3.3) (1.1.28)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from azure-ai-formrecognizer==3.3.3) (4.11.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /home/codespace/.local/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-formrecognizer==3.3.3) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /home/codespace/.local/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-formrecognizer==3.3.3) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.3.3) (2024.2.2)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.3.3) (0.6.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.3.3) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-formrecognizer==3.3.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-formrecognizer==3.3.3) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-formrecognizer==3.3.3) (2.0.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.21->azure-ai-formrecognizer==3.3.3) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install \"azure-ai-formrecognizer==3.3.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Azure Forms Recognizer, Azure Cognitive Search, OpenAI, and other python modules\n",
    "\n",
    "import os, json, requests, sys, re\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient \n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SearchIndex,\n",
    "    SearchField,\n",
    "    SearchFieldDataType,\n",
    "    SimpleField,\n",
    "    SearchableField,\n",
    "    SemanticConfiguration,\n",
    "    PrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSettings\n",
    ")\n",
    "\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "import openai\n",
    "import numpy as np\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is secure and recommended way to load OpenAI resource credentials and deployment names\n",
    "\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "openai.api_base = os.environ['OPENAI_API_BASE']\n",
    "openai.api_type = os.environ['OPENAI_API_TYPE']\n",
    "openai.api_version = os.environ['OPENAI_API_VERSION']\n",
    "\n",
    "chat_model = os.environ['CHAT_MODEL_NAME']\n",
    "embedding_model=os.environ['EMBEDDING_MODEL_NAME']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The path in the code cell below is referring to the `/data/unstructured/raw` folder. You may need to update this path if you are running this notebook from a different location then from where you extracted it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- raw data\n",
    "RAW_DATA_FOLDER= '../data/unstructured/raw'\n",
    "# -- extracted json file \n",
    "EXTRACTED_DATA_FOLDER = '../data/unstructured/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.formrecognizer import DocumentAnalysisClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_FORM_RECOGNIZER_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_FORM_RECOGNIZER_KEY\"]\n",
    "\n",
    "document_analysis_client = DocumentAnalysisClient(\n",
    "    endpoint=endpoint, credential=AzureKeyCredential(key)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to extract the data from our unstructured data into a more readable format for the model to understand. The Form Recognizer tool helps us do so by leveraging the prebuilt layout models. Here, we primarily are working with PDFs but we could also have JPG and PNG formats that the form recognizer tool also supports.\n",
    "\n",
    "For each document, we want to specify the way information is being extracted. For example in this use case, each document has many pages. To keep track of the pages, we store them in page_number. We also want to extract the content for each page and drop it in a page_context field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_local_single_file(file_name: str):\n",
    "    not_completed = True\n",
    "    while not_completed:\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            poller = document_analysis_client.begin_analyze_document(\n",
    "                \"prebuilt-layout\", document=f\n",
    "            )\n",
    "            not_completed=False\n",
    "    result = poller.result()\n",
    "    return get_page_content(file_name, result)\n",
    "\n",
    "def extract_files( folder_name: str, destination_folder_name: str):\n",
    "    os.makedirs(destination_folder_name, exist_ok=True)\n",
    "    for file in os.listdir(folder_name):\n",
    "        if file[-3:].upper() in ['PDF','JPG','PNG']:\n",
    "            print('Processing file:', file, end='')\n",
    "        \n",
    "            page_content = extract_local_single_file(os.path.join(folder_name, file))\n",
    "            output_file = os.path.join(destination_folder_name, file[:-3] +'json')\n",
    "            print(f'  write output to {output_file}')\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(json.dumps(page_content))\n",
    "\n",
    "\n",
    "def get_page_content(file_name:str, result):\n",
    "    page_content = []\n",
    "    for page in result.pages:\n",
    "        all_lines_content = []\n",
    "        for line_idx, line in enumerate(page.lines):\n",
    "            all_lines_content.append(' '.join([word.content for word in line.get_words()]))\n",
    "        page_content.append({'page_number':page.page_number, \n",
    "                                'page_content':' '.join(all_lines_content)})\n",
    "    return {'filename':file_name, 'content':page_content}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: AutoPrompt_Eliciting_Knowledge_From_LanguageModels.pdf"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  write output to ../data/unstructured/extracted/AutoPrompt_Eliciting_Knowledge_From_LanguageModels.json\n",
      "Processing file: Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.pdf  write output to ../data/unstructured/extracted/Chain-of-Thought_Prompting_Elicits_Reasoning_in_LLMs.json\n",
      "Processing file: Generated_Knowledge_Prompting_for_Commonsense_Reasoning.pdf  write output to ../data/unstructured/extracted/Generated_Knowledge_Prompting_for_Commonsense_Reasoning.json\n",
      "Processing file: Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.pdf  write output to ../data/unstructured/extracted/Self-Consistency_Improves_Chain-of-Thought_Reasonsing_in_LLMs.json\n",
      "Processing file: Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf  write output to ../data/unstructured/extracted/Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.json\n",
      "Processing file: Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.pdf  write output to ../data/unstructured/extracted/Power_of_Scale_for_Parameter-Efficient_Prompt_Tuning.json\n",
      "Processing file: Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.pdf  write output to ../data/unstructured/extracted/Prefix-Tuning_Optimizing_Continuous_Prompts_for_Generation.json\n",
      "Processing file: LLMs_are_Human-Level_Prompt_Engineers.pdf  write output to ../data/unstructured/extracted/LLMs_are_Human-Level_Prompt_Engineers.json\n"
     ]
    }
   ],
   "source": [
    "extract_files(RAW_DATA_FOLDER, EXTRACTED_DATA_FOLDER)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More About our data\n",
    "\n",
    "For this walkthrough, we will take a look at various Research Papers on LLM topics in PDF documents. This includes topics like autoprompting, chain of thought prompting, precise zero shot dense retrival, and more. This dataset contains various unstructured formats such as text, tables, graphs, and formulas.\n",
    "\n",
    "## Data Description\n",
    "\n",
    "The relevant schema for our work today consists of \n",
    "\n",
    "- document_id\n",
    "- document_name\n",
    "- file_path\n",
    "- page_number\n",
    "- page_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[]\n",
    "for file in os.listdir(EXTRACTED_DATA_FOLDER):\n",
    "    with open(os.path.join(EXTRACTED_DATA_FOLDER, file)) as f:\n",
    "        page_content= json.loads(f.read())\n",
    "    documents.extend(\n",
    "        [\n",
    "            {\n",
    "                'document_id':page_content['filename'].split('/')[-1].split('.')[0] + '-' + str(page['page_number']),\n",
    "                'document_name':page_content['filename'].split('/')[-1],\n",
    "                'file_path':page_content['filename'],              \n",
    "                'page_number':page['page_number'],\n",
    "                'page_text':page['page_content']\n",
    "            }\n",
    "            for page in page_content['content']\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document_id': 'Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels-2',\n",
       " 'document_name': 'Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf',\n",
       " 'file_path': '../data/unstructured/raw/Precise_Zero-Shot_Dense_Retrieval_without_Relevance_Labels.pdf',\n",
       " 'page_number': 2,\n",
       " 'page_text': 'write a passage to answer the question how long does it take to remove wisdom tooth HyDE It usually takes between 30 minutes and two hours to remove a wisdom tooth ... write a scientific paper passage to answer the question How has the COVID-19 pandemic impacted mental health? How wisdom teeth are removed ... Some ... a few minutes, whereas others can take 20 minutes or longer .... .. depression and anxiety had increased by 20% since the start of the pandemic ... 인 간 이 불 을 사 용 한 기 록 은 약 800 만 년 전 부 터 나 타 난 다 ... GPT Contriever ... two studies investigating >COVID-19 patients ... significantly higher level of depressive ... write a passage in Korean to answer the question in detail 인 간 은 언 제 불 을 사 용 했 는 가 ? 불 을 처 음 사 용 한 시 기 는 호 모 에 렉 투 스 가 살 았 던 142 만 년 전 으 로 거 슬 러 간 다 ... instruction query generated document real document Figure 1: An illustration of the HyDE model. Documents snippets are shown. HyDE serves all types of queries without changing the underlying GPT-3 and Contriever/mContriever models. to human intent to follow instructions. With these ingredients, we propose to pivot through Hypothetical Document Embeddings (HyDE), and decompose dense retrieval into two tasks, a generative task per- formed by an instruction-following language model and a document-document similarity task performed by a contrastive encoder (Figure 1). First, we feed the query to the generative model and instruct it to \"write a document that answers the question\", i.e. a hypothetical document. We expect the generative process to capture \"relevance\" by giving an example; the generated document is not real, can contain factual errors but is like a relevant document. In the second step, we use an unsupervised contrastive encoder to encode this document into an embedding vector. Here, we expect the encoder\\'s dense bottleneck to serve a lossy compressor, where the extra (hallucinated) details are filtered out from the embedding. We use this vector to search against the corpus embeddings. The most similar real documents are retrieved and returned. The retrieval leverages document-document similarity encoded in the inner-product during contrastive training. Note that, interestingly, with HyDE factorization, the query-document similarity score is no longer explicitly modeled nor computed. Instead, the retrieval task is cast into two NLU and NLG tasks. HyDE appears unsupervised. No model is trained in HyDE: both the generative model and the con- trastive encoder remain intact. Supervision signals were only involved in instruction learning of our backbone LLM. In our experiments, we show HyDE using Instruct- GPT (Ouyang et al., 2022) and Contriever (Izacard et al., 2021) as backbone models significantly out- performs the previous state-of-the-art Contriever- only zero-shot no-relevance system on 11 queries sets, covering tasks like Web Search, Question Answering, Fact Verification and languages like Swahili, Korean, Japanese. 2 Related Works Dense Retrieval (Lee et al., 2019; Karpukhin et al., 2020) has been extensively studied after the emergence of pre-trained Transformer language models (Devlin et al., 2019). Researchers stud- ied the metric learning problems, such as training loss (Karpukhin et al., 2020) and negative sam- pling (Xiong et al., 2021; Qu et al., 2021), and also introduced distillation (Qu et al., 2021; Lin et al., 2021b; Hofstätter et al., 2021). Later works studied the second stage pre-training of language model specifically for retrieval (Izacard et al., 2021; Gao and Callan, 2021; Lu et al., 2021; Gao and Callan, 2022; Liu and Shao, 2022). The popularity of dense retrieval can be partially attributed to the rich and successful research in very efficient minimum inner product search (MIPS) at very large (billion) scales (Johnson et al., 2017). Instructions-Following Language Models Soon after the emergence of LLMs, several groups of researchers discover that LLMs trained on data consisting of instructions and their execution can zero-shot generalize to perform new tasks with new instructions (Ouyang et al., 2022; Sanh et al., 2022; Min et al., 2022; Wei et al., 2022). This can be done by standard supervised sequence-to-sequence learning or more effectively with reinforcement learning (Ouyang et al., 2022). Concurrent to us, Asai et al. (2022) studied \"Task-aware Retrieval with Instructions\". They fine-tuned dense encoders that can also encode task-specific instruction prepended to query. In comparison, we use an unsupervised encoder and handle different tasks and their instruction with an'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of a single page of research paper file that will be indexed in Azure Cognitive Search\n",
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will focus on Cognitive Search and the following topics:\n",
    "1. Creating an index client\n",
    "2. Defining the index fields with necessary attributes\n",
    "3. Creating a semantic configuration\n",
    "4. Loading our index with the document pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.search.documents.indexes._search_index_client.SearchIndexClient at 0x7e2975873880>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an SDK client\n",
    "service_endpoint = os.getenv(\"AZURE_COGNITIVE_SEARCH_ENDPOINT\")   \n",
    "key = os.getenv(\"AZURE_COGNITIVE_SEARCH_KEY\")\n",
    "credential = AzureKeyCredential(key)\n",
    "\n",
    "index_name = \"research-paper-index\"\n",
    "\n",
    "index_client = SearchIndexClient(\n",
    "    endpoint=service_endpoint, credential=credential)\n",
    "index_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " research-paper-index created\n"
     ]
    }
   ],
   "source": [
    "fields = [\n",
    "    SimpleField(name=\"document_id\", type=SearchFieldDataType.String, key=True),\n",
    "    SimpleField(name=\"page_number\", type=SearchFieldDataType.Int64),\n",
    "    SimpleField(name=\"file_path\", type=SearchFieldDataType.String),\n",
    "    SearchableField(name=\"document_name\", type=SearchFieldDataType.String,\n",
    "                searchable=True, retrievable=True),\n",
    "    SearchableField(name=\"page_text\", type=SearchFieldDataType.String,\n",
    "                filterable=True, searchable=True, retrievable=True),\n",
    "]\n",
    "\n",
    "semantic_config = SemanticConfiguration(\n",
    "    name=\"my-semantic-config\",\n",
    "    prioritized_fields=PrioritizedFields(\n",
    "        title_field=SemanticField(field_name=\"document_id\"),\n",
    "        prioritized_keywords_fields=[SemanticField(field_name=\"document_name\")],\n",
    "        prioritized_content_fields=[SemanticField(field_name=\"page_text\")]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Create the semantic settings with the configuration\n",
    "semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "# Create the search index with the semantic settings\n",
    "index = SearchIndex(name=index_name, fields=fields, semantic_settings=semantic_settings)\n",
    "result = index_client.create_or_update_index(index)\n",
    "print(f' {result.name} created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded 179 documents\n"
     ]
    }
   ],
   "source": [
    "search_client = SearchClient(endpoint=service_endpoint, index_name=index_name, credential=credential)\n",
    "result = search_client.upload_documents(documents)  \n",
    "print(f\"Uploaded {len(documents)} documents\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see Azure Cognitive Search in action! We can retrive the most relevant documents out of all the ones that we are working with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is automated prompt engineering?\"\n",
    "count = 10\n",
    "results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "page_chunks = []\n",
    "citations = []\n",
    "for result in results:\n",
    "    page_chunks.append(result['page_text'])\n",
    "    citations.append(result['document_name'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question Tracy used a piece of wire 4 feet lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction Only In-context Only Instruction +...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original Input xinp a real joy. AUTOPROMPT Xpr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>However, writing prompts is not only time cons...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Task CSQA2 Generate some knowledge about the i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Task NumerSense Prompt Generate some numerical...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...\n",
       "1  LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...\n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...\n",
       "3  Question Tracy used a piece of wire 4 feet lon...\n",
       "4  Instruction Only In-context Only Instruction +...\n",
       "5  Original Input xinp a real joy. AUTOPROMPT Xpr...\n",
       "6  However, writing prompts is not only time cons...\n",
       "7  Table 24: Few-shot exemplars for full chain of...\n",
       "8  Task CSQA2 Generate some knowledge about the i...\n",
       "9  Task NumerSense Prompt Generate some numerical..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df = pd.DataFrame(page_chunks, columns = [\"page_chunks\"]) #datframe with document chunks\n",
    "embed_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the most relevant documents, let us create embeddings for all the page chunks. This will help us find the most similar documents to our given user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Rate Limits\n",
    "\n",
    "from openai.error import RateLimitError\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_embedding(text: str, engine: str = \"text-embedding-ada-002\"):\n",
    "    count=0\n",
    "    while True:\n",
    "        try:\n",
    "            embedding = openai.Embedding().create(input=[text], engine=engine)[\"data\"][0][\"embedding\"]\n",
    "            break;\n",
    "        except RateLimitError:\n",
    "            count+=1\n",
    "            #print(f'RateLimitError Count: {count}')\n",
    "            sleep(2)            \n",
    "    return np.array(embedding).astype(np.float32)\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-35-turbo\"): \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "      <td>[-0.024442429, 0.0011039738, 0.006048606, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...</td>\n",
       "      <td>[-0.019849496, -0.003594275, 0.00012190554, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AUTOPROMPT: Eliciting Knowledge from Language ...</td>\n",
       "      <td>[-0.020375008, 0.0042142053, 0.00047743105, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Question Tracy used a piece of wire 4 feet lon...</td>\n",
       "      <td>[0.012769195, 0.0060897674, 0.0153582785, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction Only In-context Only Instruction +...</td>\n",
       "      <td>[-0.023537232, 0.011187449, 0.017725568, -0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original Input xinp a real joy. AUTOPROMPT Xpr...</td>\n",
       "      <td>[-0.027736554, -0.014863948, 0.004185375, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>However, writing prompts is not only time cons...</td>\n",
       "      <td>[-0.038262248, -0.001478378, -0.004975918, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Table 24: Few-shot exemplars for full chain of...</td>\n",
       "      <td>[0.013370869, 0.012658401, 0.033202365, -0.005...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Task CSQA2 Generate some knowledge about the i...</td>\n",
       "      <td>[0.017976638, 0.030040093, 0.038017314, -0.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Task NumerSense Prompt Generate some numerical...</td>\n",
       "      <td>[-0.00255838, 0.012899804, 0.032524493, -0.004...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...   \n",
       "1  LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...   \n",
       "2  AUTOPROMPT: Eliciting Knowledge from Language ...   \n",
       "3  Question Tracy used a piece of wire 4 feet lon...   \n",
       "4  Instruction Only In-context Only Instruction +...   \n",
       "5  Original Input xinp a real joy. AUTOPROMPT Xpr...   \n",
       "6  However, writing prompts is not only time cons...   \n",
       "7  Table 24: Few-shot exemplars for full chain of...   \n",
       "8  Task CSQA2 Generate some knowledge about the i...   \n",
       "9  Task NumerSense Prompt Generate some numerical...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.024442429, 0.0011039738, 0.006048606, -0.0...  \n",
       "1  [-0.019849496, -0.003594275, 0.00012190554, -0...  \n",
       "2  [-0.020375008, 0.0042142053, 0.00047743105, -0...  \n",
       "3  [0.012769195, 0.0060897674, 0.0153582785, -0.0...  \n",
       "4  [-0.023537232, 0.011187449, 0.017725568, -0.00...  \n",
       "5  [-0.027736554, -0.014863948, 0.004185375, -0.0...  \n",
       "6  [-0.038262248, -0.001478378, -0.004975918, -0....  \n",
       "7  [0.013370869, 0.012658401, 0.033202365, -0.005...  \n",
       "8  [0.017976638, 0.030040093, 0.038017314, -0.006...  \n",
       "9  [-0.00255838, 0.012899804, 0.032524493, -0.004...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_chunks</th>\n",
       "      <th>embedding</th>\n",
       "      <th>similarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A PROMPT ENGINEERING IN THE WILD Large models ...</td>\n",
       "      <td>[-0.024442429, 0.0011039738, 0.006048606, -0.0...</td>\n",
       "      <td>0.890529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...</td>\n",
       "      <td>[-0.019849496, -0.003594275, 0.00012190554, -0...</td>\n",
       "      <td>0.865939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>However, writing prompts is not only time cons...</td>\n",
       "      <td>[-0.038262248, -0.001478378, -0.004975918, -0....</td>\n",
       "      <td>0.844472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         page_chunks  \\\n",
       "0  A PROMPT ENGINEERING IN THE WILD Large models ...   \n",
       "1  LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUM...   \n",
       "2  However, writing prompts is not only time cons...   \n",
       "\n",
       "                                           embedding  similarities  \n",
       "0  [-0.024442429, 0.0011039738, 0.006048606, -0.0...      0.890529  \n",
       "1  [-0.019849496, -0.003594275, 0.00012190554, -0...      0.865939  \n",
       "2  [-0.038262248, -0.001478378, -0.004975918, -0....      0.844472  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embedding = get_embedding(query, engine=embedding_model)\n",
    "embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "top_results = (\n",
    "    embed_df.sort_values(\"similarities\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    "    .head(3)\n",
    ")\n",
    "top_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
      "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
      "\n",
      "User Query: ```What is automated prompt engineering?```\n",
      "List of Extracted Pages: ```['A PROMPT ENGINEERING IN THE WILD Large models with natural language interfaces, including models for text generation and image synthesis, have seen an increasing amount of public usage in recent years. As finding the right prompt can be difficult for humans, a number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed. Among others, see, for example: · https://blog.andrewcantino.com/blog/2021/04/21/prompt-engineering-tips-and-tricks/ . https://techcrunch.com/2022/07/29/a-startup-is-charging-1-99-for-strings-of-text-to-feed-to-dall-e-2/ . https://news.ycombinator.com/item?id=32943224 . https://promptomania.com/stable-diffusion-prompt-builder/ . https://huggingface.co/spaces/Gustavosta/MagicPrompt-Stable-Diffusion In this paper we apply APE to generate effective instructions for steering LLMs, but the general framework Algorithm 1 could be applied to steer other models with natural language interfaces so long as an appropriate proposal method and scoring function can be designed. 15', 'LARGE LANGUAGE PROMPT ENGINEERS MODELS ARE HUMAN-LEVEL Yongchao Zhou*,1,2 Andrei Ioan Muresanu*,2,3 Ziwen Han*,1,2 Keiran Paster1,2 Silviu Pitis1,2 Harris Chan1,2 Jimmy Ba1,2 1 University of Toronto 2 Vector Institute 3 University of Waterloo ABSTRACT By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the \"program,\" optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer. arXiv:2211.01910v1 [cs.LG] 3 Nov 2022 1 INTRODUCTION The combination of scale and attention-based architectures has resulted in language models possessing an unprecedented level of generality (Kaplan et al., 2020; Vaswani et al., 2017). These so-called \"large language models\" (LLMs) have shown remarkable, often superhuman, capabilities across a diverse range of tasks, including both zero-shot and few-shot setups (Brown et al., 2020; Srivastava et al., 2022). With generality, however, there comes a question of control: how can we make LLMs do what we want them to do? To answer this question and steer LLMs toward desired behaviors, recent work has considered fine-tuning (Ouyang et al., 2022; Ziegler et al., 2019), in-context learning (Brown et al., 2020), and several forms of prompt generation (Gao, 2021), including both differentiable tuning of soft prompts (Qin & Eisner, 2021; Lester et al., 2021) and natural language prompt engineering (Reynolds & McDonell, 2021). The latter is of particular interest, as it provides a natural interface for humans to communicate with machines and may be of great relevance not only to LLMs but to other generalist models such as prompted image synthesizers (Rombach et al., 2022; Ramesh et al., 2022), for which public interest in prompt design and generation has also emerged (see Appendix A for examples). Behind this interest is the fact that plain language prompts do not always produce the desired results, even when those results are possible to produce with alternative instructions. Thus, human users must experiment with a wide range of prompts to elicit desired behaviors, as they have little knowledge of how compatible instructions are with a particular model. We can understand this by viewing LLMs as black-box computers that execute programs specified by natural language instructions: while they can execute a broad range of natural language programs, the way these programs are processed may not be intuitive for humans, and the quality of instruction can only be measured when executing these instructions on a downstream task (Sanh et al., 2022; Wei et al., 2021). To reduce the human effort involved in creating and validating effective instructions, we propose a novel algorithm using LLMs to generate and select instructions automatically. We call this problem * Equal contribution. Corresponding email: yczhou@cs.toronto.edu 1 Our code is available at https : //github. com/keirp/automatic_prompt_engineer. 1', 'However, writing prompts is not only time consum- ing, but it is not clear that the same phrasing will be effective for every model, nor is it clear what crite- ria determine whether a particular phrasing the best to elicit the desired information. In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1 An illustration of AUTOPROMPT is provided in Figure 1. The prompt is constructed by taking the original task inputs-a collection of one or more sequences of tokens (e.g., the review in Figure 1)-and mapping them to a sequence of tokens using a template. In the following sections, we describe how AUTOPROMPT uses labeled train- ing data to construct prompts, and how it uses the output of the MLM as a prediction for the task. 2.1 Background and Notation For the purpose of prompt construction, we distin- guish the original task inputs xinp (e.g., the review in Figure 1, \"a real joy.\") from the prompt prompt (e.g., \"a real joy. atmosphere alot dialogue Clone totally [MASK].\") that is fed into the MLM. The mapping from xinp to @prompt is performed using a template, A. This template defines where each input sequence will be placed in the prompt, as well as the placement of any additional tokens. In particular, it must also define the placement of a special [MASK] token for the MLM to fill in (de- noted by [P] in the template to distinguish it from other [MASK] tokens that might appear). Feeding the prompt into the MLM produces a probability distribution p([MASK]|@prompt) describing which tokens most likely fill in the blank. If class labels naturally correspond to tokens in the vocabulary (e.g., entity names in knowledge base completion tasks), this distribution may be readily interpreted as a distribution over class la- bels. However, for tasks such as sentiment analysis, there may be a set of label tokens Vy that corre- spond to a particular label y. For example, in Fig- ure 1, \"Cris\", \"marvelous\", and \"philanthrop\" all indicate positive sentiment. In this case, the class probability is obtained by marginalizing over the 1 Although we focus only on MLMs in this work, our method is trivially extendable to autoregressive LMs. The only adjustment is that the predict token must occur at the end of the prompt. set of label tokens: p(y|@prompt) = > p([MASK] = w| @prompt) WEVy (1) 2.2 Gradient-Based Prompt Search So far, we have shown how to reformulate a clas- sification task as a language modeling task using prompts. Here, we propose a method for automatic prompt construction based on Wallace et al. (2019). The idea is to add a number of \"trigger\" tokens that are shared across all prompts (denoted by [T] in the example template in Figure 1). These tokens are initialized to [MASK] tokens, and then iteratively updated to maximize the label likelihood (Equa- tion (1)) over batches of examples. Formally, at each step, we compute a first-order approximation of the change in the log-likelihood that would be produced by swapping the jth trigger token ring with another token w E V. Then we identify a candidate set Vcand of the top-k tokens estimated to cause the greatest increase: Vcand = top-k [win V log p(y|xprompt)] WEV (2) where Win is the input embedding of w, and the gradient is taken with respect to the input embed- ding of xtrig. Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model (the dot-products require the same amount of multiplications as com- puting the LM output projection). For each candi- date in this set, we then re-evaluate Equation (1) on the updated prompt, and retain the prompt with the highest probability in the next step-this requires k forward passes of the model. An example prompt produced by this method for the task of sentiment analysis is shown in Figure 1. 2.3 Automating Label Token Selection While in some settings the choice of label tokens is obvious (e.g., when class labels directly correspond to words in the vocabulary), it is less clear what label tokens are appropriate for problems involv- ing more abstract class labels (e.g., NLI). In this section, we develop a general two-step approach to automate the selection of the sets of label tokens Vy. In the first step, we train a logistic classifier to predict the class label using the contextualized embedding of the [MASK] token as input: h = Transformerenc () (3)']```\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "User Query: ```{query}```\n",
    "List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering refers to the process of automatically generating effective instructions or prompts for large language models (LLMs) with natural language interfaces. This is done by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. The quality of the selected instruction is evaluated by evaluating the zero-shot performance of another LLM following the selected instruction. A number of guides on prompt engineering as well as tools to aid in prompt discovery have been developed to aid in this process. The goal of automated prompt engineering is to reduce the human effort involved in creating and validating effective instructions, and to steer LLMs toward desired behaviors.\n"
     ]
    }
   ],
   "source": [
    "response = get_completion(prompt, chat_model)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_search(query, count=10):\n",
    "    results = search_client.search(search_text=query, top=count, include_total_count=True)\n",
    "    page_chunks = []\n",
    "    for result in results:\n",
    "        page_chunks.append(result['page_text'])\n",
    "        \n",
    "    #Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\n",
    "    embed_df['embedding'] = embed_df[\"page_chunks\"].apply(lambda page_text : get_embedding(page_text, engine = embedding_model))\n",
    "\n",
    "    query_embedding = get_embedding(query, engine=embedding_model)\n",
    "    embed_df[\"similarities\"] = embed_df['embedding'].apply(lambda page_embedding: cosine_similarity(page_embedding, query_embedding))\n",
    "\n",
    "    top_results = (\n",
    "        embed_df.sort_values(\"similarities\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "        .head(3)\n",
    "    )\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Provided below are user query and list of extracted pages from research papers separated by triple backticks.\n",
    "    Your task is to extract key pieces of information from that list based on the user query and phrase that as a comprehensive answer. \n",
    "\n",
    "    User Query: ```{query}```\n",
    "    List of Extracted Pages: ```{top_results['page_chunks'].to_list()}```\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_completion(prompt, chat_model)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated prompt engineering involves using large language models (LLMs) to generate and select natural language instructions (prompts) for specific tasks. This is done by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. The quality of the selected instruction is evaluated by evaluating the zero-shot performance of another LLM following the selected instruction. This method reduces the human effort involved in creating and validating effective instructions. Various tools and guides have been developed to aid in prompt discovery.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"How does automated prompt engineering work?\", 5)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tuning refers to the process of finding the right prompt for large models with natural language interfaces, such as those used for text generation and image synthesis. This can be difficult for humans, but there are guides and tools available to aid in prompt discovery. One such tool is AUTOPROMPT, an automated method for generating prompts for any task based on a gradient-guided search. AUTOPROMPT has been shown to be effective in numerous experiments, including constructing prompts that test pretrained masked language models (MLMs) on sentiment analysis and natural language inference (NLI), and fact retrieval tasks of LAMA, where it elicits more accurate factual knowledge from MLMs than manually created prompts. AUTOPROMPT is a viable parameter-free alternative to existing probing methods and potentially a replacement for finetuning.\n"
     ]
    }
   ],
   "source": [
    "answer = query_search(\"what is prompt tuning?\", 10)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
